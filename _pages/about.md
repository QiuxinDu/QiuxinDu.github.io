---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

# ğŸ‘©â€ğŸ“ About Me
<p style="line-height: 1.7">I am a full-time Research Assistant at <a href="https://www.comp.nus.edu.sg/" target="_blank" style="color: #0b2cd0;text-decoration: none">NUS SOC</a>, advised by <a href="https://www.comp.nus.edu.sg/cs/people/wangye/" target="_blank" style="color: #0b2cd0;text-decoration: none">Prof. Ye Wang</a>. Previously, I am a research intern at HKUST, supervised by <a href="https://scholar.google.com/citations?user=DCRUAmAAAAAJ&amp;hl=en" target="_blank" style="color: #0b2cd0;text-decoration: none">Xiaojuan Ma</a>. I obtained master's degree in the VR Lab at <a href="https://www.bit.edu.cn/" target="_blank" style="    color: #0b2cd0;text-decoration: none">Beijing Institute of Technology (BIT)</a>, advised by <a href="https://scholar.google.com/citations?user=DCRUAmAAAAAJ&amp;hl=en" target="_blank" style="color: #0b2cd0;text-decoration: none">Prof. Dongdong Weng</a>, majoring in Virtual Reality and Human-Computer Interaction. Prior to master, I obtained bachelor's degree in Computer Science and technology at <a href="http://www.zzu.edu.cn/" target="_blank" style="color: #0b2cd0;text-decoration: none">Zhengzhou University</a> in 2021.</p>

<p style="line-height: 1.7">I attended the HCIX Research Club and became a Research Assistant at the <a href="https://apex-group-hkust.netlify.app/" target="_blank" style="color: #0b2cd0;text-decoration: none">APEX Lab of HKUST</a> in 2023, supervised by <a href="https://www.mingmingfan.com/" target="_blank" style="color: #0b2cd0;text-decoration: none">Prof. Mingming Fan</a>, independently supporting a HCI project. Before this, I was an intern in Light Illusions supervised by <a href="https://ece.hkust.edu.hk/pingtan" target="_blank" style="color: #0b2cd0;text-decoration: none">Prof. Ping Tan</a> from June to August in 2023.</p>

<p style="line-height: 1.7">My current research interest lies in <strong style="color: #0b2cd0">Human-AI Collaboration for Digital Health</strong>, focusing on inferring health status by detecting human behaviors and physiological signals, and providing appropriate digital interventions.</p> 


# ğŸ”¥ News

- *2025.04*: &nbsp;ğŸ‰ğŸ‰ I became a full-time Research Assistant at <a href="https://www.comp.nus.edu.sg/" target="_blank" style="color: #0b2cd0;text-decoration: none">NUS SOC</a>, advised by <a href="https://www.comp.nus.edu.sg/cs/people/wangye/" target="_blank" style="color: #0b2cd0;text-decoration: none">Prof. Ye Wang</a>.  
- *2024.12*: &nbsp;ğŸ‰ğŸ‰ Our work AI-administered Neurocognitive Disorder Screening submitted to <a href="https://www.sciencedirect.com/journal/international-journal-of-human-computer-studies" target="_blank" style="color: #0b2cd0;text-decoration: none">IJHCS</a>.
- *2024.10*: &nbsp;ğŸ‰ğŸ‰ Our work <a href="https://link.springer.com/chapter/10.1007/978-981-96-3679-2_1" target="_blank" style="color: #0b2cd0;text-decoration: none">Motion Generation Review</a> was accepted by <a href="https://icxr.net/2024/index.html" target="_blank" style="color: #0b2cd0;text-decoration: none">ICXR 2024</a>. 
- *2024.09*: &nbsp;ğŸ‰ğŸ‰ Our work <a href="https://dl.acm.org/doi/abs/10.1145/3710924" target="_blank" style="color: #0b2cd0;text-decoration: none">AI as a Bridge Across Ages</a> was accepted by <a href="https://cscw.acm.org/2025/" target="_blank" style="color: #0b2cd0;text-decoration: none">CSCW 2025</a>.  
- *2024.07*: &nbsp;ğŸ‰ğŸ‰ One paper submitted to <a href="https://www.tandfonline.com/action/authorSubmission?show=instructions&journalCode=hihc20" target="_blank" style="color: #0b2cd0;text-decoration: none">IJHCI</a>. 
- *2024.05*: &nbsp;ğŸ‰ğŸ‰ I have spent a wonderful week at <a href="https://ieeevr.org/2023/" target="_blank" style="color: #0b2cd0;text-decoration: none">CHI 2024</a> in Hawaii. It was my pleasure to present <a href="https://dl.acm.org/doi/abs/10.1145/3613904.3642187" target="_blank" style="color: #0b2cd0;text-decoration: none">my work</a> as well as meet many new friends!
- *2024.01*: &nbsp;ğŸ‰ğŸ‰ Our work <a href="https://dl.acm.org/doi/abs/10.1145/3613904.3642187" target="_blank" style="color: #0b2cd0;text-decoration: none">LightSword</a> was accepted by <a href="https://chi2024.acm.org/" target="_blank" style="color: #0b2cd0;text-decoration: none">CHI 2024</a>. 
- *2024.01*: &nbsp;ğŸ‰ğŸ‰ One paper submitted to <a href="https://cscw.acm.org/2024/" target="_blank" style="color: #0b2cd0;text-decoration: none">CSCW 2024</a>. 
- *2023.03*: &nbsp;ğŸ‰ğŸ‰ I have spent a wonderful week at <a href="https://ieeevr.org/2023/" target="_blank" style="color: #0b2cd0;text-decoration: none">IEEE VR 2023</a> in Shanghai.
- *2022.08*: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by <a href="https://ismar2022.vgtc.org/" target="_blank" style="color: #0b2cd0;text-decoration: none">ISMAR 2022</a> as a poster.


# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CHI 2024</div><img src='images/CHI2024.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**LightSword: A Customized Virtual Reality Exergame for Long-Term Cognitive Inhibition Training in Older Adults** 
  
<p style="margin-top: 20px;">CHI 2024</p>


<p style="margin-top: 20px;"><strong style="color: #0b2cd0">Qiuxin Du</strong>, Zhen Song, Haiyan Jiang, Xiaoying Wei, Dongdong Weng, Mingming Fan</p>

<p style="margin-top: 20px;">Developed an age-friendly VR exergame using Unity based on cognitive activation paradigms and adaptive difficulty algorithms. Conducted an eight-month user study with 12 older adults, demonstrating that the game improved cognitive performance in older adults, with benefits lasting for six months.</p>

<p style="margin-top: 20px;"><a href="assets\pdf\CHI2024_CognitiveTraining.pdf" target="_blank" style="color: #0b2cd0;text-decoration: none">Paper</a> | <a href="https://drive.google.com/drive/home" target="_blank" style="color: #0b2cd0;text-decoration: none">Video</a> | Code | BibTex <strong><span class="show_paper_citations" data="3WQTKocAAAAJ:WF5omc3nYNoC"></span></strong></p>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CSCW 2025</div><img src='images/CSCW2024.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**AI as a Bridge Across Ages: Exploring The Opportunities of Artificial Intelligence in Supporting Inter-Generational Communication in Virtual Reality** 
  
<p style="margin-top: 10px;">CSCW 2025</p>


<p style="margin-top: 10px;"><strong style="color: #0b2cd0">Qiuxin Du*</strong>, Xiaoying Wei*, Jiawei Li, Jie Hao, Emily Kuang, Dongdong Weng, Mingming Fan</p>

<p style="margin-top: 20px;">We developed a AI-powered VR game using UE aiming at bridging the gaps in inter-generational communication, which integrated the following AI features: LLM for the silence and awkwardness in communication, Text2Image for easy to express and easy to learn, Emotion visualization in VR avatars for bridging subtle emotional expressions and facilitating the development of closer relationships, Scene generation for enhanced focus and mood regulation. And then we conducted a probe-based participatory design study with twelve inter-generational pairs. </p>

<p style="margin-top: 20px;"><a href="https://arxiv.org/abs/2410.17909" target="_blank" style="color: #0b2cd0;text-decoration: none">Paper</a> | <a href="https://drive.google.com/drive/home" target="_blank" style="color: #0b2cd0;text-decoration: none">Video</a> | Code | BibTex <strong><span class="show_paper_citations" data="3WQTKocAAAAJ:WF5omc3nYNoC"></span></strong></p>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICXR 2024</div><img src='images/motion.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Motion Generation Review: Exploring Deep Learning for Lifelike Animation with Manifold** 
  
<p style="margin-top: 10px;">ICXR 2024</p>


<p style="margin-top: 10px;">Jiayi Zhao, Dongdong Weng*, <strong style="color: #0b2cd0">Qiuxin Du</strong>, Zeyu Tian</p>

<p style="margin-top: 20px;">Human motion generation involves creating natural sequences of human body poses, aiming to produce lifelike virtual characters with realistic movements, enhancing virtual agents and immersive experiences. We present a comprehensive overview of manifold applications in human motion generation, and explore methods for extracting manifolds from unstructured data, their application in motion generation, and discuss their advantages and future directions. </p>

<p style="margin-top: 20px;"> Paper | Video | Code | BibTex <strong><span class="show_paper_citations" data="3WQTKocAAAAJ:WF5omc3nYNoC"></span></strong></p>

</div>
</div>


# ğŸ“– Projects



<div class='paper-box'><div class='paper-box-image'><div><img src='images/manyou.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Exploration of Game Roaming in Virtual Environments: The Interplay of Posture, Interaction Freedom, and Locomotion** 

<p style="margin-top: 8px;">Submitted to IJHCI</p>

<p style="margin-top: 8px;">Jie Hao, <strong style="color: #0b2cd0">Qiuxin Du</strong>, Ming Li, Bin Liang, Jie Guo, Dongdong Weng, Yongtian Wang</p>

<p style="margin-top: 8px;">Using SSQ data and ECG signals to investigate the effects of body posture, interaction freedom, and locomotion method on user experience and cognitive load in VR, focusing on cybersickness, sense of presence, and workload.</p>

<p style="margin-top: 10px;">Paper | <a href="https://drive.google.com/drive/home" target="_blank" style="color: #0b2cd0;text-decoration: none">Video</a> | Code | BibTex <strong><span class="show_paper_citations" data="3WQTKocAAAAJ:WF5omc3nYNoC"></span></strong></p>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src='assets/gif/eva.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Real-time High-fidelity Intelligent Virtual Agents** 


<p style="margin-top: 8px;"><strong style="color: #0b2cd0">Qiuxin Du</strong>, Yihua Bao, Xiaonuo Dongye, Qinbo Yu, Zeyu Tian, Zhihe Zhao, Dongdong Weng</p>

<p style="margin-top: 8px;">To achieve realistic appearances, proposed a deep learning-based method for automatic generation of digital human expressions and movements; for intelligent dialogue, integrated LLM into the system.</p> 
<p style="margin-top: 8px;">Representative works: <a href="https://space.bilibili.com/1943410799?spm_id_from=333.337.0.0" target="_blank" style="color: #0b2cd0;text-decoration: none">Lydia</a> for Byte Dance, Digital <a href="https://www.bilibili.com/video/BV1Pq4y1c7bw/?spm_id_from=333.337.search-card.all.click&vd_source=cc9abae04a88edf06d4e63651c404199" target="_blank" style="color: #0b2cd0;text-decoration: none">Mei Lanfang</a>, and Sign Language Virtual Agents.</p>

<p style="margin-top: 8px;">Win National Gold Award as First Author with our work</p>
</div>
</div>


# ğŸ– Honors and Awards
- *2024.07*  Outstanding Graduate in Beijing and BIT.
- *2023.06*  Northern Industry Scholarship (only 26 students per year in BIT, Top 0.1%).
- *2023.05*  Youth Role Model in BIT (2023).
- *2023.03*  Gold Award in the 13th Challenge Cup.
- *2022.08*  Second Prize in The 8th Internet\+.
- *2021.07*  Outstanding Graduate in ZZU.
- *2018.12*  Honor Student in Henan Provinces.





# ğŸ’» Internships
- *2023.06 - 2023.08*, [Light Illusions](https://www.lightillusion.com/), China.